{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fleanend/TorchGlocalK/blob/main/Glocal_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ty3gYQgtnwFA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Glocal_K' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/usydnlp/Glocal_K.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl2tU6kL8Ot3",
    "outputId": "379fe638-9ba3-4513-c9d0-6bbd279e03e2"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.sparse import csc_matrix\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "torch.manual_seed(1284)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4A9uU1WloQ2"
   },
   "source": [
    "# Data Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cq3KEUaVo1o3"
   },
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_train):\n",
    "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_8kEkg9mlIW"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0fkA1WpmipzF"
   },
   "outputs": [],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = ''\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJqSSY33mgkw",
    "outputId": "54f7ca43-b9f7-4edb-8628-783a4513f4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "try:\n",
    "    path = data_path + 'Glocal_K/data/MovieLens_100K/'\n",
    "    n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
    "except Exception:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nGCdp_FlobOK"
   },
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500 # size of hidden layers\n",
    "n_dim = 5 # inner AE embedding size\n",
    "n_layers = 2 # number of hidden layers\n",
    "gk_size = 3 # width=height of kernel for convolution\n",
    "\n",
    "# Hyperparameters to tune for specific case\n",
    "max_epoch_p = 500 # max number of epochs for pretraining\n",
    "max_epoch_f = 1000 # max number of epochs for finetuning\n",
    "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10 # and finetuning\n",
    "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-5 # and finetuning\n",
    "lambda_2 = 20. # regularisation of number or parameters\n",
    "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1 # dot product weight for global kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sWtU4-pmDDT"
   },
   "source": [
    "# Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "p1P6fgYiy28F"
   },
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "    dist = torch.norm(u - v, p=2, dim=2)\n",
    "    hat = torch.clamp(1. - dist**2, min=0.)\n",
    "    return hat\n",
    "\n",
    "class KernelLayer(nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
    "      super().__init__()\n",
    "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
    "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
    "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
    "      self.b = nn.Parameter(torch.randn(n_hid))\n",
    "\n",
    "      self.lambda_s = lambda_s\n",
    "      self.lambda_2 = lambda_2\n",
    "\n",
    "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      nn.init.zeros_(self.b)\n",
    "      self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "      w_hat = local_kernel(self.u, self.v)\n",
    "    \n",
    "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
    "      sparse_reg_term = self.lambda_s * sparse_reg\n",
    "      \n",
    "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
    "      l2_reg_term = self.lambda_2 * l2_reg\n",
    "\n",
    "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
    "      y = torch.matmul(x, W_eff) + self.b\n",
    "      y = self.activation(y)\n",
    "\n",
    "      return y, sparse_reg_term + l2_reg_term\n",
    "\n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
    "      super().__init__()\n",
    "      layers = []\n",
    "      for i in range(n_layers):\n",
    "        if i == 0:\n",
    "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
    "        else:\n",
    "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
    "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
    "      self.layers = nn.ModuleList(layers)\n",
    "      self.dropout = nn.Dropout(0.33)\n",
    "\n",
    "    def forward(self, x):\n",
    "      total_reg = None\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        x, reg = layer(x)\n",
    "        if i < len(self.layers)-1:\n",
    "          x = self.dropout(x)\n",
    "        if total_reg is None:\n",
    "          total_reg = reg\n",
    "        else:\n",
    "          total_reg += reg\n",
    "      return x, total_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7RGKh1ckXgtP"
   },
   "outputs": [],
   "source": [
    "class CompleteNet(nn.Module):\n",
    "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
    "      super().__init__()\n",
    "      self.gk_size = gk_size\n",
    "      self.dot_scale = dot_scale\n",
    "      self.local_kernel_net = kernel_net\n",
    "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
    "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
    "      \n",
    "\n",
    "    def forward(self, x, x_local):\n",
    "      gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
    "      x = self.global_conv(x, gk)\n",
    "      x, global_reg_loss = self.local_kernel_net(x)\n",
    "      return x, global_reg_loss\n",
    "\n",
    "    def global_kernel(self, input, gk_size, dot_scale):\n",
    "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
    "      avg_pooling = avg_pooling.view(1, -1)\n",
    "\n",
    "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
    "      gk = gk.view(1, 1, gk_size, gk_size)\n",
    "\n",
    "      return gk\n",
    "\n",
    "    def global_conv(self, input, W):\n",
    "      input = input.unsqueeze(0).unsqueeze(0)\n",
    "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
    "      return conv2d.squeeze(0).squeeze(0)\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
    "      # L2 loss\n",
    "      diff = train_m * (train_r - pred_p)\n",
    "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
    "      loss_p = sqE + reg_loss\n",
    "      return loss_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8sQCwrSmKG4"
   },
   "source": [
    "# Network Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOtWj1SCo1RW"
   },
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7teUrgWagpW0"
   },
   "outputs": [],
   "source": [
    "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IEBsNhNo4Cj"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OiTXqnN6zLXQ"
   },
   "outputs": [],
   "source": [
    "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sETwz58aK6y6"
   },
   "source": [
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vyReXxgac3KH"
   },
   "outputs": [],
   "source": [
    "def dcg_k(score_label, k):\n",
    "    dcg, i = 0., 0\n",
    "    for s in score_label:\n",
    "        if i < k:\n",
    "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jwsSR-8ZdGWo"
   },
   "outputs": [],
   "source": [
    "def ndcg_k(y_hat, y, k):\n",
    "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
    "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
    "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
    "    norm, i = 0., 0\n",
    "    for s in score_label_:\n",
    "        if i < k:\n",
    "            norm += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    dcg = dcg_k(score_label, k)\n",
    "    return dcg / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "yy9eQS51pbhj"
   },
   "outputs": [],
   "source": [
    "def call_ndcg(y_hat, y):\n",
    "    ndcg_sum, num = 0, 0\n",
    "    y_hat, y = y_hat.T, y.T\n",
    "    n_users = y.shape[0]\n",
    "\n",
    "    for i in range(n_users):\n",
    "        y_hat_i = y_hat[i][np.where(y[i])]\n",
    "        y_i = y[i][np.where(y[i])]\n",
    "\n",
    "        if y_i.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
    "        num += 1\n",
    "\n",
    "    return ndcg_sum / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXXQjeMxmYEC"
   },
   "source": [
    "# Training and Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ35Zoha-Eue",
    "outputId": "4fc0c647-b0a5-4e69-afff-899c14dc247d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 0 test rmse: 2.7598052 train rmse: 2.7419877\n",
      "Time: 0.5730128288269043 seconds\n",
      "Time cumulative: 0.5730128288269043 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 50 test rmse: 1.0235298 train rmse: 0.9848115\n",
      "Time: 3.3552627563476562 seconds\n",
      "Time cumulative: 99.4905731678009 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 100 test rmse: 0.96286654 train rmse: 0.91974497\n",
      "Time: 6.152752637863159 seconds\n",
      "Time cumulative: 338.4643461704254 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 146 test rmse: 0.957368 train rmse: 0.913806\n",
      "Time: 8.66681694984436 seconds\n",
      "Time cumulative: 672.9148526191711 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
     ]
    }
   ],
   "source": [
    "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
    "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
    "\n",
    "time_cumulative = 0\n",
    "tic = time()\n",
    "\n",
    "# Pre-Training\n",
    "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  x = torch.Tensor(train_r).double().to(device)\n",
    "  m = torch.Tensor(train_m).double().to(device)\n",
    "  complete_model.local_kernel_net.train()\n",
    "  pred, reg = complete_model.local_kernel_net(x)\n",
    "  loss = Loss().to(device)(pred, reg, m, x)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_p):\n",
    "  optimizer.step(closure)\n",
    "  complete_model.local_kernel_net.eval()\n",
    "  t = time() - tic\n",
    "  time_cumulative += t\n",
    "\n",
    "  pre, _ = model(torch.Tensor(train_r).double().to(device))\n",
    "  \n",
    "  pre = pre.float().cpu().detach().numpy()\n",
    "  \n",
    "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "  test_rmse = np.sqrt(error)\n",
    "\n",
    "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "  train_rmse = np.sqrt(error_train)\n",
    "\n",
    "  if last_rmse-train_rmse < tol_p:\n",
    "    counter += 1\n",
    "  else:\n",
    "    counter = 0\n",
    "\n",
    "  last_rmse = train_rmse\n",
    "\n",
    "  if patience_p == counter:\n",
    "    print('.-^-._' * 12)\n",
    "    print('PRE-TRAINING')\n",
    "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "    print('Time:', t, 'seconds')\n",
    "    print('Time cumulative:', time_cumulative, 'seconds')\n",
    "    print('.-^-._' * 12)\n",
    "    break\n",
    "\n",
    "\n",
    "  if i % 50 != 0:\n",
    "    continue\n",
    "  print('.-^-._' * 12)\n",
    "  print('PRE-TRAINING')\n",
    "  print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "  print('Time:', t, 'seconds')\n",
    "  print('Time cumulative:', time_cumulative, 'seconds')\n",
    "  print('.-^-._' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6v_tODcweLn",
    "outputId": "5ab3f058-5c7c-458a-c3b5-32bbcf4df418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 0 test rmse: 0.9873072 test mae: 0.7971504 test ndcg: 0.8851623567661495\n",
      "Epoch: 0 train rmse: 0.94342536 train mae: 0.7642855 train ndcg: 0.8915047892390016\n",
      "Time: 8.789841651916504 seconds\n",
      "Time cumulative: 681.7046942710876 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 50 test rmse: 0.91453725 test mae: 0.7211846 test ndcg: 0.8986689343894995\n",
      "Epoch: 50 train rmse: 0.84911346 train mae: 0.6699192 train ndcg: 0.9091132912807548\n",
      "Time: 27.029043912887573 seconds\n",
      "Time cumulative: 1576.433170557022 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 100 test rmse: 0.9099648 test mae: 0.71713495 test ndcg: 0.8990776675355154\n",
      "Epoch: 100 train rmse: 0.84560674 train mae: 0.6668781 train ndcg: 0.9095044627628056\n",
      "Time: 45.32638454437256 seconds\n",
      "Time cumulative: 3398.160195350647 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 150 test rmse: 0.9086819 test mae: 0.7179778 test ndcg: 0.901491777820803\n",
      "Epoch: 150 train rmse: 0.8424021 train mae: 0.66615635 train ndcg: 0.9118363472756155\n",
      "Time: 63.62783145904541 seconds\n",
      "Time cumulative: 6133.02216386795 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 200 test rmse: 0.90838206 test mae: 0.7147741 test ndcg: 0.9003243479276719\n",
      "Epoch: 200 train rmse: 0.83989424 train mae: 0.66159666 train ndcg: 0.911755010172645\n",
      "Time: 82.78969168663025 seconds\n",
      "Time cumulative: 9799.465899705887 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 250 test rmse: 0.9091555 test mae: 0.7163603 test ndcg: 0.9000655967921274\n",
      "Epoch: 250 train rmse: 0.8386267 train mae: 0.66201365 train ndcg: 0.912117893064287\n",
      "Time: 101.64135432243347 seconds\n",
      "Time cumulative: 14420.73621726036 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 300 test rmse: 0.90916663 test mae: 0.7168659 test ndcg: 0.8992425946097985\n",
      "Epoch: 300 train rmse: 0.83821166 train mae: 0.6615601 train ndcg: 0.9116470601656304\n",
      "Time: 119.67105388641357 seconds\n",
      "Time cumulative: 19955.395669221878 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 350 test rmse: 0.90990263 test mae: 0.71722746 test ndcg: 0.9000802732802338\n",
      "Epoch: 350 train rmse: 0.8372831 train mae: 0.66107583 train ndcg: 0.9131659302312801\n",
      "Time: 138.17673563957214 seconds\n",
      "Time cumulative: 26410.02724671364 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 400 test rmse: 0.9093042 test mae: 0.71650076 test ndcg: 0.8997941067026971\n",
      "Epoch: 400 train rmse: 0.8360716 train mae: 0.6598385 train ndcg: 0.9127439196538856\n",
      "Time: 156.81749629974365 seconds\n",
      "Time cumulative: 33792.0196928978 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 450 test rmse: 0.90892416 test mae: 0.7173528 test ndcg: 0.899392942232668\n",
      "Epoch: 450 train rmse: 0.8359555 train mae: 0.6608893 train ndcg: 0.9137381964259018\n",
      "Time: 174.87097144126892 seconds\n",
      "Time cumulative: 42103.92236185074 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 500 test rmse: 0.9084346 test mae: 0.71538824 test ndcg: 0.9001914585017061\n",
      "Epoch: 500 train rmse: 0.8344778 train mae: 0.6580811 train ndcg: 0.9135986619892017\n",
      "Time: 193.10891222953796 seconds\n",
      "Time cumulative: 51308.172187805176 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 550 test rmse: 0.90846866 test mae: 0.71535176 test ndcg: 0.8996497455155233\n",
      "Epoch: 550 train rmse: 0.833795 train mae: 0.65737444 train ndcg: 0.913601538150399\n",
      "Time: 211.5612292289734 seconds\n",
      "Time cumulative: 61426.121638298035 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 600 test rmse: 0.9082609 test mae: 0.7143646 test ndcg: 0.899202953299566\n",
      "Epoch: 600 train rmse: 0.83305764 train mae: 0.65648645 train ndcg: 0.9135733893796661\n",
      "Time: 229.4091763496399 seconds\n",
      "Time cumulative: 72461.97115397453 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 650 test rmse: 0.90875256 test mae: 0.71572864 test ndcg: 0.8998665848488688\n",
      "Epoch: 650 train rmse: 0.83155054 train mae: 0.65626067 train ndcg: 0.9144053231926551\n",
      "Time: 248.84334230422974 seconds\n",
      "Time cumulative: 84415.07431077957 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 700 test rmse: 0.9081162 test mae: 0.715979 test ndcg: 0.8989377926020661\n",
      "Epoch: 700 train rmse: 0.8297375 train mae: 0.6553161 train ndcg: 0.9146812584511798\n",
      "Time: 267.29522681236267 seconds\n",
      "Time cumulative: 97313.10181593895 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 750 test rmse: 0.90827733 test mae: 0.7154525 test ndcg: 0.8991057106083601\n",
      "Epoch: 750 train rmse: 0.82761467 train mae: 0.6533739 train ndcg: 0.915422719700384\n",
      "Time: 286.1632454395294 seconds\n",
      "Time cumulative: 111154.68712091446 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 800 test rmse: 0.908257 test mae: 0.71633816 test ndcg: 0.8992582152427873\n",
      "Epoch: 800 train rmse: 0.82572776 train mae: 0.6521218 train ndcg: 0.9160827374362289\n",
      "Time: 304.58718252182007 seconds\n",
      "Time cumulative: 125936.3586204052 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 850 test rmse: 0.90899736 test mae: 0.7166741 test ndcg: 0.8997852534546671\n",
      "Epoch: 850 train rmse: 0.8242694 train mae: 0.6511756 train ndcg: 0.9168976409711012\n",
      "Time: 323.70807790756226 seconds\n",
      "Time cumulative: 141649.0448155403 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 900 test rmse: 0.9101145 test mae: 0.71841604 test ndcg: 0.899293613689646\n",
      "Epoch: 900 train rmse: 0.8240321 train mae: 0.6522329 train ndcg: 0.91646126456127\n",
      "Time: 341.7566192150116 seconds\n",
      "Time cumulative: 158299.28137993813 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 950 test rmse: 0.9091552 test mae: 0.716221 test ndcg: 0.8990333523676242\n",
      "Epoch: 950 train rmse: 0.8224469 train mae: 0.64947253 train ndcg: 0.9170865814251115\n",
      "Time: 359.89549136161804 seconds\n",
      "Time cumulative: 175843.3120558262 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuning\n",
    "\n",
    "train_r_local = np.clip(pre, 1., 5.)\n",
    "\n",
    "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  x = torch.Tensor(train_r).double().to(device)\n",
    "  x_local = torch.Tensor(train_r_local).double().to(device)\n",
    "  m = torch.Tensor(train_m).double().to(device)\n",
    "  complete_model.train()\n",
    "  pred, reg = complete_model(x, x_local)\n",
    "  loss = Loss().to(device)(pred, reg, m, x)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(max_epoch_f):\n",
    "  optimizer.step(closure)\n",
    "  complete_model.eval()\n",
    "  t = time() - tic\n",
    "  time_cumulative += t\n",
    "\n",
    "  pre, _ = complete_model(torch.Tensor(train_r).double().to(device), torch.Tensor(train_r_local).double().to(device))\n",
    "  \n",
    "  pre = pre.float().cpu().detach().numpy()\n",
    "\n",
    "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "  test_rmse = np.sqrt(error)\n",
    "\n",
    "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "  train_rmse = np.sqrt(error_train)\n",
    "\n",
    "  test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
    "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
    "\n",
    "  test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
    "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
    "\n",
    "  if test_rmse < best_rmse:\n",
    "      best_rmse = test_rmse\n",
    "      best_rmse_ep = i+1\n",
    "\n",
    "  if test_mae < best_mae:\n",
    "      best_mae = test_mae\n",
    "      best_mae_ep = i+1\n",
    "\n",
    "  if best_ndcg < test_ndcg:\n",
    "      best_ndcg = test_ndcg\n",
    "      best_ndcg_ep = i+1\n",
    "\n",
    "  if last_rmse-train_rmse < tol_f:\n",
    "    counter += 1\n",
    "  else:\n",
    "    counter = 0\n",
    "\n",
    "  last_rmse = train_rmse\n",
    "\n",
    "  if patience_f == counter:\n",
    "    print('.-^-._' * 12)\n",
    "    print('FINE-TUNING')\n",
    "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
    "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
    "    print('Time:', t, 'seconds')\n",
    "    print('Time cumulative:', time_cumulative, 'seconds')\n",
    "    print('.-^-._' * 12)\n",
    "    break\n",
    "\n",
    "\n",
    "  if i % 50 != 0:\n",
    "    continue\n",
    "\n",
    "  print('.-^-._' * 12)\n",
    "  print('FINE-TUNING')\n",
    "  print('Epoch:', i, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
    "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
    "  print('Time:', t, 'seconds')\n",
    "  print('Time cumulative:', time_cumulative, 'seconds')\n",
    "  print('.-^-._' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTi_PdXJqTjh",
    "outputId": "6f5a2d63-2c1f-446f-8f20-d4357d4bdc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 736  best rmse: 0.9069864\n",
      "Epoch: 881  best mae: 0.7135806\n",
      "Epoch: 150  best ndcg: 0.9015076990954262\n"
     ]
    }
   ],
   "source": [
    "# Final result\n",
    "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
    "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
    "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6Yfh3hm4Efa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "glocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
